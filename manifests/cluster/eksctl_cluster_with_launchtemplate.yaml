apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_DEFAULT_REGION}
  version: "${K8S_VERSION}"
  tags:
    cluster-autoscaler: enabled

# Kubernetes 网络配置
kubernetesNetworkConfig:
  serviceIPv4CIDR: "${SERVICE_IPV4_CIDR}"

# 使用已经存在的vpc
vpc:
  id: "${VPC_ID}"
  subnets:
    private:
      ${AZ_A}:
        id: "${PRIVATE_SUBNET_A}"
      ${AZ_B}:
        id: "${PRIVATE_SUBNET_B}"
      ${AZ_C}:
        id: "${PRIVATE_SUBNET_C}"
    public:
      ${AZ_A}:
        id: "${PUBLIC_SUBNET_A}"
      ${AZ_B}:
        id: "${PUBLIC_SUBNET_B}"
      ${AZ_C}:
        id: "${PUBLIC_SUBNET_C}"
  clusterEndpoints:
    privateAccess: true
    publicAccess: false

accessConfig:
  authenticationMode: API_AND_CONFIG_MAP

# IAM 配置 - 使用 Pod Identity
iam:
  withOIDC: false

# 节点组配置 - 使用 additionalVolumes + userData
managedNodeGroups:
  - name: eks-utils-arm64
    instanceType: m8g.large
    amiFamily: AmazonLinux2023
    desiredCapacity: 3
    minSize: 3
    maxSize: 6
    volumeSize: 50
    volumeType: gp3

    # 额外数据盘
    additionalVolumes:
      - volumeName: '/dev/xvdb'
        volumeSize: 100
        volumeType: gp3
        volumeIOPS: 3000
        volumeThroughput: 125

    privateNetworking: true
    subnets:
      - ${PRIVATE_SUBNET_A}
      - ${PRIVATE_SUBNET_B}
      - ${PRIVATE_SUBNET_C}
    labels:
      app: "eks-utils"
      arch: "arm64"
      node-group-type: "system"
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/${CLUSTER_NAME}: "owned"

    # Pre-bootstrap user data - 在 EKS bootstrap 之前执行
    preBootstrapCommands:
      - |
        #!/bin/bash
        set -ex

        # Log to file for debugging
        exec > >(tee /var/log/lvm-setup.log)
        exec 2>&1

        echo "=== Starting LVM Setup at $$(date) ==="

        # Stop containerd
        systemctl stop containerd || true

        # Wait for data disk to be available (max 60 seconds)
        echo "Waiting for data disk..."
        for i in {1..60}; do
          DISK=$$(lsblk -dpno NAME | grep nvme | grep -v nvme0n1 | head -1)
          if [ -n "$$DISK" ]; then
            echo "Found data disk: $$DISK"
            break
          fi
          echo "Attempt $$i/60: Data disk not found yet, waiting..."
          sleep 1
        done

        if [ -z "$$DISK" ]; then
          echo "ERROR: No data disk found after 60 seconds"
          systemctl start containerd
          exit 0
        fi

        # Check if LVM already configured
        if vgs vg_data &>/dev/null; then
          echo "LVM already configured, mounting..."
          mount /dev/vg_data/lv_containerd /var/lib/containerd || true
          systemctl start containerd
          exit 0
        fi

        # Install lvm2
        echo "Installing lvm2..."
        dnf install -y lvm2

        # Create LVM
        echo "Creating LVM on $$DISK..."
        pvcreate "$$DISK"
        vgcreate vg_data "$$DISK"
        lvcreate -l 100%VG -n lv_containerd vg_data
        mkfs.xfs /dev/vg_data/lv_containerd

        # Mount and migrate data
        echo "Mounting and migrating containerd data..."
        mkdir -p /mnt/runtime/containerd
        mount /dev/vg_data/lv_containerd /mnt/runtime/containerd
        rsync -aHAX /var/lib/containerd/ /mnt/runtime/containerd/ || true
        umount /mnt/runtime/containerd
        mount /dev/vg_data/lv_containerd /var/lib/containerd

        # Add to fstab
        grep -q "lv_containerd" /etc/fstab || \
          echo "/dev/vg_data/lv_containerd /var/lib/containerd xfs defaults,nofail 0 2" >> /etc/fstab

        echo "LVM setup completed successfully"
        df -h /var/lib/containerd
        vgs
        lvs

        # Start containerd
        systemctl start containerd

        echo "=== LVM Setup Complete at $$(date) ==="

addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
    configurationValues: |
      nodeSelector:
        app: eks-utils
  - name: kube-proxy
    version: latest
  - name: eks-pod-identity-agent
    version: latest
  - name: aws-ebs-csi-driver
    version: latest

cloudWatch:
  clusterLogging:
    logRetentionInDays: 30
    enableTypes:
      - "api"
      - "audit"
      - "authenticator"
      - "controllerManager"
      - "scheduler"
