apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_DEFAULT_REGION}
  version: "${K8S_VERSION}"
  tags:
    cluster-autoscaler: enabled

# Kubernetes 网络配置
kubernetesNetworkConfig:
  serviceIPv4CIDR: "${SERVICE_IPV4_CIDR}"

# 使用已经存在的vpc
vpc:
  id: "${VPC_ID}"
  subnets:
    private:
      ${AZ_A}:
        id: "${PRIVATE_SUBNET_A}"
      ${AZ_B}:
        id: "${PRIVATE_SUBNET_B}"
      ${AZ_C}:
        id: "${PRIVATE_SUBNET_C}"
    public:
      ${AZ_A}:
        id: "${PUBLIC_SUBNET_A}"
      ${AZ_B}:
        id: "${PUBLIC_SUBNET_B}"
      ${AZ_C}:
        id: "${PUBLIC_SUBNET_C}"
  clusterEndpoints:
    privateAccess: true
    publicAccess: false

accessConfig:
  authenticationMode: API_AND_CONFIG_MAP

# IAM 配置 - 使用 Pod Identity
iam:
  withOIDC: false

# 节点组配置 - 使用 Launch Template with LVM
managedNodeGroups:
  - name: eks-utils-arm64
    instanceType: m8g.large
    amiFamily: AmazonLinux2023
    desiredCapacity: 3
    minSize: 3
    maxSize: 6
    privateNetworking: true
    subnets:
      - ${PRIVATE_SUBNET_A}
      - ${PRIVATE_SUBNET_B}
      - ${PRIVATE_SUBNET_C}
    labels:
      app: "eks-utils"
      arch: "arm64"
      node-group-type: "system"
    tags:
      k8s.io/cluster-autoscaler/enabled: "true"
      k8s.io/cluster-autoscaler/${CLUSTER_NAME}: "owned"

    # Launch Template 配置
    launchTemplate:
      # 使用 overrideBootstrapCommand 完全控制启动流程
      overrideBootstrapCommand: |
        #!/bin/bash
        set -ex

        # === LVM Setup (before bootstrap) ===
        echo "=== Starting LVM Setup ==="

        # Stop containerd
        systemctl stop containerd || true

        # Auto-detect EBS data disk (exclude root disk nvme0n1)
        DISK=$(lsblk -dpno NAME | grep nvme | grep -v nvme0n1 | head -1)

        if [ -z "$DISK" ]; then
          echo "No data disk found, skip LVM setup"
        else
          echo "Found data disk: $DISK"

          # Check if LVM already configured
          if ! vgs vg_data &>/dev/null; then
            # Install lvm2 (not installed by default on AL2023)
            dnf install -y lvm2

            # Create LVM
            pvcreate "$DISK"
            vgcreate vg_data "$DISK"
            lvcreate -l 100%VG -n lv_containerd vg_data
            mkfs.xfs /dev/vg_data/lv_containerd

            # Mount LV to temporary directory and migrate data
            mkdir -p /mnt/runtime/containerd
            mount /dev/vg_data/lv_containerd /mnt/runtime/containerd

            # Copy containerd data
            rsync -aHAX /var/lib/containerd/ /mnt/runtime/containerd/ || true

            # Unmount temp and mount to final destination
            umount /mnt/runtime/containerd
            mount /dev/vg_data/lv_containerd /var/lib/containerd

            # Add to fstab
            echo "/dev/vg_data/lv_containerd /var/lib/containerd xfs defaults,nofail 0 2" >> /etc/fstab

            echo "LVM setup completed"
            df -h /var/lib/containerd
          else
            echo "LVM already configured"
            mount /dev/vg_data/lv_containerd /var/lib/containerd || true
          fi
        fi

        # Start containerd
        systemctl start containerd

        # === EKS Bootstrap ===
        /etc/eks/bootstrap.sh ${CLUSTER_NAME} \
          --kubelet-extra-args '--node-labels=app=eks-utils,arch=arm64,node-group-type=system'

      # Block device mappings
      blockDeviceMappings:
        # Root volume
        - deviceName: /dev/xvda
          ebs:
            volumeSize: 50
            volumeType: gp3
            encrypted: true
            deleteOnTermination: true
        # Data volume for containerd
        - deviceName: /dev/xvdb
          ebs:
            volumeSize: 100
            volumeType: gp3
            encrypted: true
            deleteOnTermination: true

addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
    configurationValues: |
      nodeSelector:
        app: eks-utils
  - name: kube-proxy
    version: latest
  - name: eks-pod-identity-agent
    version: latest
  - name: aws-ebs-csi-driver
    version: latest

cloudWatch:
  clusterLogging:
    logRetentionInDays: 30
    enableTypes:
      - "api"
      - "audit"
      - "authenticator"
      - "controllerManager"
      - "scheduler"
