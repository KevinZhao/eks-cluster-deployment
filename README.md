# EKS é›†ç¾¤è‡ªåŠ¨åŒ–éƒ¨ç½²å®Œæ•´æŒ‡å—

ç”Ÿäº§çº§ AWS EKS é›†ç¾¤è‡ªåŠ¨åŒ–éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒæ ‡å‡†éƒ¨ç½²å’Œè‡ªå®šä¹‰ Launch Template éƒ¨ç½²ã€‚

[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.34-326CE5?logo=kubernetes)](https://kubernetes.io/)
[![AWS](https://img.shields.io/badge/AWS-EKS-FF9900?logo=amazon-aws)](https://aws.amazon.com/eks/)

---

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ¶æ„è¯´æ˜](#æ¶æ„è¯´æ˜)
- [éƒ¨ç½²æ–¹å¼é€‰æ‹©](#éƒ¨ç½²æ–¹å¼é€‰æ‹©)
- [å‰ç½®è¦æ±‚](#å‰ç½®è¦æ±‚)
- [æ ‡å‡†éƒ¨ç½²](#æ ‡å‡†éƒ¨ç½²)
- [Launch Template éƒ¨ç½²](#launch-template-éƒ¨ç½²)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [æˆæœ¬ä¼˜åŒ–](#æˆæœ¬ä¼˜åŒ–)
- [éªŒè¯å’Œæµ‹è¯•](#éªŒè¯å’Œæµ‹è¯•)
- [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
  - [å¦‚ä½•ä» VPC å¤–éƒ¨ç½²é›†ç¾¤](#å¦‚ä½•ä»-vpc-å¤–éƒ¨ç½²é›†ç¾¤)
- [æ¸…ç†èµ„æº](#æ¸…ç†èµ„æº)

---

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒåŠŸèƒ½
- âœ… **è‡ªåŠ¨åŒ–éƒ¨ç½²** - ä¸€é”®éƒ¨ç½²å®Œæ•´ EKS é›†ç¾¤
- âœ… **å¤š AZ é«˜å¯ç”¨** - è·¨ 3 ä¸ªå¯ç”¨åŒºéƒ¨ç½²
- âœ… **æ··åˆæ¶æ„** - ç³»ç»ŸèŠ‚ç‚¹ Intelï¼Œåº”ç”¨èŠ‚ç‚¹ Gravitonï¼Œå…¼é¡¾å…¼å®¹æ€§å’Œæˆæœ¬
- âœ… **å·¥ä½œè´Ÿè½½éš”ç¦»** - ç³»ç»Ÿç»„ä»¶å’Œåº”ç”¨å®Œå…¨éš”ç¦»
- âœ… **è‡ªåŠ¨æ‰©ç¼©å®¹** - Cluster Autoscaler è‡ªåŠ¨ç®¡ç†èŠ‚ç‚¹
- âœ… **å­˜å‚¨æ”¯æŒ** - EBS/EFS/S3 CSI Driver
- âœ… **è´Ÿè½½å‡è¡¡** - AWS Load Balancer Controller
- âœ… **è‡ªå®šä¹‰èŠ‚ç‚¹** - æ”¯æŒ Launch Template è‡ªå®šä¹‰é…ç½®ï¼ˆSSH Keyã€æ•°æ®ç›˜ã€é¢„è£…è½¯ä»¶ï¼‰

### æ”¯æŒçš„å­˜å‚¨ç±»å‹
- **EBS** (gp3) - å—å­˜å‚¨ï¼Œé€‚åˆæ•°æ®åº“
- **EFS** - å…±äº«æ–‡ä»¶ç³»ç»Ÿï¼Œé€‚åˆå¤š Pod è®¿é—®
- **S3** (Mountpoint) - å¯¹è±¡å­˜å‚¨ï¼Œé€‚åˆå¤§æ•°æ®

### å·²é›†æˆç»„ä»¶
| ç»„ä»¶ | ç‰ˆæœ¬ | ç”¨é€” |
|------|------|------|
| Kubernetes | 1.34 | å®¹å™¨ç¼–æ’ |
| VPC CNI | v1.18.5 | Pod ç½‘ç»œ |
| CoreDNS | v1.11.3 | DNS è§£æ |
| Kube-proxy | v1.31.2 | ç½‘ç»œä»£ç† |
| Pod Identity Agent | v1.3.4 | IAM è®¤è¯ |
| EBS CSI Driver | v1.37.0 | å—å­˜å‚¨ |
| Cluster Autoscaler | v1.34.2 | è‡ªåŠ¨æ‰©ç¼©å®¹ |
| AWS LB Controller | v2.11.0 | è´Ÿè½½å‡è¡¡ |

---

## âš¡ å¿«é€Ÿå¼€å§‹

### æœ€ç®€éƒ¨ç½²ï¼ˆ5 åˆ†é’Ÿé…ç½® + 20 åˆ†é’Ÿç­‰å¾…ï¼‰

```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
nano .env  # å¡«å†™ VPC_IDã€å­ç½‘ ID ç­‰

# 2. éƒ¨ç½²é›†ç¾¤
chmod +x scripts/*.sh
./scripts/4_install_eks_cluster.sh
```

**éƒ¨ç½²æ—¶é—´:** çº¦ 20-25 åˆ†é’Ÿ

> âš ï¸ **é‡è¦æç¤º**ï¼šæœ¬é¡¹ç›®é›†ç¾¤é…ç½®ä¸ºç§æœ‰ API è®¿é—®ï¼ˆ`publicAccess: false`ï¼‰ï¼Œéƒ¨ç½²è„šæœ¬éœ€è¦ä» **VPC å†…éƒ¨** æ‰§è¡Œã€‚å¦‚æœæ‚¨åœ¨ VPC å¤–éƒ¨ï¼ˆå¦‚ CloudShellã€æœ¬åœ°æœºå™¨ï¼‰ï¼Œè¯·å‚è€ƒ [å¦‚ä½•ä» VPC å¤–éƒ¨ç½²é›†ç¾¤](#å¦‚ä½•ä»-vpc-å¤–éƒ¨ç½²é›†ç¾¤) ç« èŠ‚ã€‚

---

## ğŸ—ï¸ æ¶æ„è¯´æ˜

### é›†ç¾¤æ¶æ„

```
EKS Cluster (Kubernetes 1.34)
â”œâ”€â”€ Control Plane (AWS æ‰˜ç®¡)
â”‚   â””â”€â”€ API Server (å†…ç½‘è®¿é—®)
â”‚
â”œâ”€â”€ eks-utils èŠ‚ç‚¹ç»„ (3x m7i.large, Intel)
â”‚   â”œâ”€â”€ æ—  Taint
â”‚   â””â”€â”€ è¿è¡Œ: CoreDNS, Cluster Autoscaler, LB Controller, CSI Controllers
â”‚
â””â”€â”€ app èŠ‚ç‚¹ç»„ (3x c8g.large, Graviton ARM64)
    â”œâ”€â”€ Taint: workload=user-apps:NoSchedule
    â””â”€â”€ è¿è¡Œ: ç”¨æˆ·åº”ç”¨ï¼ˆéœ€è¦ tolerationï¼‰
```

### ç½‘ç»œæ¶æ„

```
VPC (10.0.0.0/16)
â”œâ”€â”€ 3ä¸ªå¯ç”¨åŒº
â”‚   â”œâ”€â”€ Public Subnet â†’ IGW
â”‚   â”‚   â””â”€â”€ NAT Gateway
â”‚   â””â”€â”€ Private Subnet â†’ NAT GW
â”‚       â””â”€â”€ EKS èŠ‚ç‚¹
```

---

## ğŸ¯ éƒ¨ç½²æ–¹å¼é€‰æ‹©

### æ–¹å¼ 1: æ ‡å‡†éƒ¨ç½² â­ æ¨èæ–°æ‰‹

**ç‰¹ç‚¹:**
- âœ… é…ç½®ç®€å•ï¼Œåªéœ€ .env æ–‡ä»¶
- âœ… å¿«é€Ÿéƒ¨ç½²ï¼Œçº¦ 20 åˆ†é’Ÿ
- âŒ æ— æ³•è‡ªå®šä¹‰ SSH Key
- âŒ æ— æ³•æ·»åŠ æ•°æ®ç›˜
- âŒ æ— æ³•è‡ªå®šä¹‰ User Data

**é€‚ç”¨:** æµ‹è¯•ç¯å¢ƒã€å­¦ä¹ ã€æ¼”ç¤º

**å‘½ä»¤:**
```bash
./scripts/4_install_eks_cluster.sh
```

### æ–¹å¼ 2: Launch Template éƒ¨ç½² â­ æ¨èç”Ÿäº§

**ç‰¹ç‚¹:**
- âœ… å®Œå…¨è‡ªå®šä¹‰èŠ‚ç‚¹é…ç½®
- âœ… æ”¯æŒ SSH Keyï¼ˆå¦‚ spider.pemï¼‰
- âœ… æ”¯æŒé¢å¤–æ•°æ®ç›˜ï¼ˆå¦‚ 1000GBï¼‰
- âœ… æ”¯æŒè‡ªå®šä¹‰ User Dataï¼ˆé¢„è£…è½¯ä»¶ã€ç³»ç»Ÿä¼˜åŒ–ï¼‰
- âœ… Terraform ç®¡ç†ï¼ŒçŠ¶æ€å¯è¿½è¸ª

**é€‚ç”¨:** ç”Ÿäº§ç¯å¢ƒã€éœ€è¦ SSHã€éœ€è¦æ•°æ®ç›˜ã€éœ€è¦é¢„è£…è½¯ä»¶

**å‘½ä»¤:**
```bash
./scripts/6_install_eks_with_custom_nodegroup.sh
```

### å¯¹æ¯”è¡¨æ ¼

| ç‰¹æ€§ | æ ‡å‡†éƒ¨ç½² | Launch Template |
|------|---------|----------------|
| å¤æ‚åº¦ | â­ ç®€å• | â­â­â­ ä¸­ç­‰ |
| SSH Key | âŒ | âœ… |
| æ•°æ®ç›˜ | âŒ | âœ… |
| é¢„è£…è½¯ä»¶ | âŒ | âœ… |
| ç³»ç»Ÿä¼˜åŒ– | âŒ | âœ… |
| éƒ¨ç½²æ—¶é—´ | 20åˆ†é’Ÿ | 25-30åˆ†é’Ÿ |

---

## ğŸ“¦ å‰ç½®è¦æ±‚

### 1. AWS ç½‘ç»œç¯å¢ƒ

**å¿…é¡»é¢„å…ˆåˆ›å»º:**
- 1 ä¸ª VPC
- 3 ä¸ªå…¬æœ‰å­ç½‘ï¼ˆæ¯ä¸ª AZï¼‰
- 3 ä¸ªç§æœ‰å­ç½‘ï¼ˆæ¯ä¸ª AZï¼‰
- NAT Gatewayï¼ˆè‡³å°‘ 1 ä¸ªï¼Œå»ºè®® 3 ä¸ªï¼‰
- Internet Gateway

**å¿«é€Ÿåˆ›å»º VPC:**
```bash
cd terraform/vpc
terraform init
terraform apply

# è·å–è¾“å‡ºç”¨äº .env
terraform output env_file_format
```

### 2. å·¥å…·è¦æ±‚

| å·¥å…· | æœ€å°ç‰ˆæœ¬ | æ£€æŸ¥å‘½ä»¤ |
|------|---------|---------|
| AWS CLI | v2.x | `aws --version` |
| eksctl | v0.150+ | `eksctl version` |
| kubectl | v1.31+ | `kubectl version --client` |
| helm | v3.x | `helm version` |
| envsubst | - | `envsubst --version` |
| terraform* | v1.0+ | `terraform version` |

*ä»… Launch Template éƒ¨ç½²éœ€è¦

**ä¸€é”®å®‰è£…ï¼ˆAmazon Linux 2023ï¼‰:**
```bash
sudo yum install -y aws-cli kubectl gettext

curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# terraformï¼ˆå¯é€‰ï¼‰
wget https://releases.hashicorp.com/terraform/1.7.0/terraform_1.7.0_linux_amd64.zip
unzip terraform_1.7.0_linux_amd64.zip
sudo mv terraform /usr/local/bin/
```

### 3. AWS æƒé™

éœ€è¦: EKSã€EC2ã€IAMã€CloudWatch Logsã€VPC æƒé™

---

## ğŸ”§ æ ‡å‡†éƒ¨ç½²

### Step 1: é…ç½®ç¯å¢ƒå˜é‡

```bash
cp .env.example .env
nano .env
```

**å¿…å¡«:**
```bash
CLUSTER_NAME=eks-demo-1
VPC_ID=vpc-xxxxxxxxxxxxxxxxx
PRIVATE_SUBNET_A=subnet-xxxxxxxxxxxxxxxxx
PRIVATE_SUBNET_B=subnet-xxxxxxxxxxxxxxxxx
PRIVATE_SUBNET_C=subnet-xxxxxxxxxxxxxxxxx
PUBLIC_SUBNET_A=subnet-xxxxxxxxxxxxxxxxx
PUBLIC_SUBNET_B=subnet-xxxxxxxxxxxxxxxxx
PUBLIC_SUBNET_C=subnet-xxxxxxxxxxxxxxxxx
AWS_REGION=ap-southeast-1
AWS_DEFAULT_REGION=ap-southeast-1
```

### Step 2: æ‰§è¡Œéƒ¨ç½²

```bash
chmod +x scripts/*.sh
./scripts/4_install_eks_cluster.sh
```

**è‡ªåŠ¨æ‰§è¡Œ:**
1. åˆ›å»º EKS é›†ç¾¤ï¼ˆ15-20åˆ†é’Ÿï¼‰
2. éƒ¨ç½² Cluster Autoscaler
3. å®‰è£… AWS Load Balancer Controller
4. è¿ç§»åˆ° Pod Identity

### Step 3: éªŒè¯

```bash
# æ£€æŸ¥èŠ‚ç‚¹ï¼ˆåº”è¯¥æœ‰ 6 ä¸ªï¼‰
kubectl get nodes -o wide

# æ£€æŸ¥ç³»ç»Ÿç»„ä»¶
kubectl get pods -n kube-system

# æ£€æŸ¥ Cluster Autoscaler
kubectl logs -n kube-system -l app=cluster-autoscaler --tail=20
```

---

## ğŸ¨ Launch Template éƒ¨ç½²

### ä½¿ç”¨åœºæ™¯ï¼šçˆ¬è™«é¡¹ç›®ç¤ºä¾‹

**éœ€æ±‚:**
- SSH Key: spider.pem
- æ•°æ®ç›˜: 1000GB
- é¢„è£…: Pythonã€çˆ¬è™«åº“ã€ç›‘æ§å·¥å…·
- ä¼˜åŒ–: æ–‡ä»¶æè¿°ç¬¦ã€TCP å‚æ•°

### Step 1: åˆ›å»º SSH Key

```bash
# åˆ›å»ºæ–° key
aws ec2 create-key-pair \
  --key-name spider \
  --region ap-southeast-1 \
  --query 'KeyMaterial' \
  --output text > spider.pem
chmod 400 spider.pem

# éªŒè¯
aws ec2 describe-key-pairs --key-names spider --region ap-southeast-1
```

### Step 2: é…ç½® Launch Template

```bash
cd terraform/launch-template

# ä½¿ç”¨ç¤ºä¾‹é…ç½®
cp terraform.tfvars.spider-example terraform.tfvars

# æŸ¥çœ‹é…ç½®
cat terraform.tfvars
```

**é…ç½®å†…å®¹ç¤ºä¾‹:**
```hcl
key_name = "spider"
instance_type = "c8g.large"

# ç³»ç»Ÿç›˜
root_volume_size = 30
root_volume_type = "gp3"

# æ•°æ®ç›˜ 1000GB
data_volume_size = 1000
data_volume_type = "gp3"
data_volume_iops = 5000
data_volume_throughput = 250

# è‡ªå®šä¹‰ User Data
custom_userdata = <<-EOT
  # æŒ‚è½½ 1000GB æ•°æ®ç›˜åˆ° /data
  if [ -e /dev/xvdb ]; then
    mkfs -t xfs /dev/xvdb
    mkdir -p /data
    mount /dev/xvdb /data
    echo '/dev/xvdb /data xfs defaults,nofail 0 2' >> /etc/fstab
  fi
  
  # å®‰è£… Python + çˆ¬è™«åº“
  yum install -y python3 python3-pip htop
  pip3 install requests beautifulsoup4 scrapy selenium
  
  # ç³»ç»Ÿä¼˜åŒ–
  echo "* soft nofile 65536" >> /etc/security/limits.conf
  echo "* hard nofile 65536" >> /etc/security/limits.conf
EOT
```

### Step 3: ä¸€é”®éƒ¨ç½²

```bash
cd ../..
./scripts/6_install_eks_with_custom_nodegroup.sh
```

**æ‰§è¡Œæµç¨‹:**
1. æ£€æŸ¥ SSH Key
2. Terraform åˆ›å»º Launch Templateï¼ˆ1-2åˆ†é’Ÿï¼‰
3. åˆ›å»º EKS åŸºç¡€é›†ç¾¤ï¼ˆ15-20åˆ†é’Ÿï¼‰
4. åˆ›å»º app èŠ‚ç‚¹ç»„ï¼ˆ5-10åˆ†é’Ÿï¼‰
5. éƒ¨ç½² Autoscaler å’Œ LB Controller

### Step 4: éªŒè¯è‡ªå®šä¹‰é…ç½®

```bash
# è·å–å®ä¾‹ ID
INSTANCE_ID=$(aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=*app-node*" \
            "Name=instance-state-name,Values=running" \
  --query 'Reservations[0].Instances[0].InstanceId' \
  --output text --region ap-southeast-1)

# ä½¿ç”¨ SSM è¿æ¥
aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1

# åœ¨èŠ‚ç‚¹ä¸ŠéªŒè¯
sudo cat /var/log/spider-node-init.log  # åˆå§‹åŒ–æ—¥å¿—
df -h /data                              # éªŒè¯æ•°æ®ç›˜
tree -L 2 /data                          # æŸ¥çœ‹ç›®å½•
python3 --version                        # éªŒè¯ Python
pip3 list | grep scrapy                  # éªŒè¯åº“
```

### Step 5: ä½¿ç”¨ SSHï¼ˆå¯é€‰ï¼‰

```bash
# ä» VPC å†…éƒ¨
NODE_IP=$(kubectl get nodes -l workload=user-apps \
  -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')

ssh -i spider.pem ec2-user@$NODE_IP
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### .env é…ç½®

**å¿…éœ€:**
```bash
CLUSTER_NAME=eks-demo-1
VPC_ID=vpc-xxx
PRIVATE_SUBNET_A=subnet-xxx
PRIVATE_SUBNET_B=subnet-xxx
PRIVATE_SUBNET_C=subnet-xxx
PUBLIC_SUBNET_A=subnet-xxx
PUBLIC_SUBNET_B=subnet-xxx
PUBLIC_SUBNET_C=subnet-xxx
AWS_REGION=ap-southeast-1
AWS_DEFAULT_REGION=ap-southeast-1
```

**å¯é€‰:**
```bash
K8S_VERSION=1.34
SERVICE_IPV4_CIDR=172.20.0.0/16
AZ_A=ap-southeast-1a
AZ_B=ap-southeast-1b
AZ_C=ap-southeast-1c
```

### Launch Template é…ç½®

**å®Œæ•´ç¤ºä¾‹ï¼ˆterraform.tfvarsï¼‰:**
```hcl
aws_region   = "ap-southeast-1"
cluster_name = "eks-demo-1"
vpc_id       = "vpc-xxx"

# SSH Key
key_name = "spider"

# å®ä¾‹
instance_type = "c8g.large"

# æ ¹å·
root_volume_size = 30
root_volume_type = "gp3"

# æ•°æ®ç›˜
data_volume_size = 1000
data_volume_type = "gp3"
data_volume_iops = 5000
data_volume_throughput = 250

# User Data
custom_userdata = <<-EOT
# ä½ çš„è‡ªå®šä¹‰è„šæœ¬
EOT
```

---

## ğŸ’° æˆæœ¬ä¼˜åŒ–

### æ ‡å‡†éƒ¨ç½²æˆæœ¬ï¼ˆæ–°åŠ å¡ï¼‰

| é¡¹ç›® | é…ç½® | æœˆåº¦ |
|------|------|------|
| EKS Control Plane | - | $72 |
| eks-utils | 3x m7i.large | $263 |
| app | 3x c8g.large | $180 |
| EBS | 6x 30GB gp3 | $18 |
| Logs | 30å¤© | $30 |
| NAT GW | 3ä¸ª | $96 |
| **æ€»è®¡** | | **$659** |

### Launch Templateï¼ˆå«1000GBæ•°æ®ç›˜ï¼‰

| é¡¹ç›® | æœˆåº¦ |
|------|------|
| åŸºç¡€ | $659 |
| **æ•°æ®ç›˜** | **+$300** |
| **æ€»è®¡** | **$959** |

### ä¼˜åŒ–å»ºè®®

**1. ä½¿ç”¨ Spot å®ä¾‹ï¼ˆèŠ‚çœ ~70%ï¼‰**
```yaml
spot: true
instanceTypes: ["c8g.large", "c7g.large"]
```
èŠ‚çœ: ~$126/æœˆ

**2. å• NAT Gateway**
```hcl
single_nat_gateway = true
```
èŠ‚çœ: $64/æœˆ

**3. å‡å°‘æ•°æ®ç›˜**
```hcl
data_volume_size = 500
```
èŠ‚çœ: $150/æœˆ

**ä¼˜åŒ–å:** $469-619/æœˆï¼ˆèŠ‚çœ 29-51%ï¼‰

---

## âœ… éªŒè¯å’Œæµ‹è¯•

### 1. éªŒè¯é›†ç¾¤

```bash
# èŠ‚ç‚¹
kubectl get nodes -o wide

# æ ‡ç­¾
kubectl get nodes --show-labels

# Taints
kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints

# Pod
kubectl get pods -A -o wide
```

### 2. æµ‹è¯• Autoscaler

```bash
# éƒ¨ç½²æµ‹è¯•
cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      tolerations:
      - key: "workload"
        operator: "Equal"
        value: "user-apps"
        effect: "NoSchedule"
      nodeSelector:
        workload: user-apps
      containers:
      - name: nginx
        image: nginx:alpine
        resources:
          requests:
            cpu: 500m
            memory: 512Mi
EOF

# æ‰©å®¹è§¦å‘è‡ªåŠ¨æ‰©å®¹
kubectl scale deployment test --replicas=10
watch kubectl get nodes

# ç¼©å®¹
kubectl scale deployment test --replicas=0

# æ¸…ç†
kubectl delete deployment test
```

### 3. æµ‹è¯• Load Balancer

```bash
# éƒ¨ç½² 2048 æ¸¸æˆ
kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.13.0/docs/examples/2048/2048_full.yaml

# ç­‰å¾… ALBï¼ˆ3-5 åˆ†é’Ÿï¼‰
kubectl get ingress -n game-2048 -w

# è®¿é—®
ALB_URL=$(kubectl get ingress -n game-2048 -o jsonpath='{.items[0].status.loadBalancer.ingress[0].hostname}')
echo "http://$ALB_URL"

# æ¸…ç†
kubectl delete namespace game-2048
```

### 4. æµ‹è¯• EBS

```bash
# åˆ›å»º PVC
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test-pvc
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp3
EOF

# æ£€æŸ¥
kubectl get pvc test-pvc

# æ¸…ç†
kubectl delete pvc test-pvc
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: é›†ç¾¤åˆ›å»ºå¤±è´¥

```bash
# æŸ¥çœ‹ CloudFormation
aws cloudformation describe-stack-events \
  --stack-name eksctl-${CLUSTER_NAME}-cluster \
  --region ${AWS_REGION}

# æ£€æŸ¥ VPC
aws ec2 describe-vpcs --vpc-ids $VPC_ID
```

### é—®é¢˜ 2: æ— æ³•è®¿é—® API Server

**é”™è¯¯:** `dial tcp 10.0.x.x:443: i/o timeout`

**åŸå› :** API çº¯å†…ç½‘è®¿é—®ï¼ˆ`publicAccess: false`ï¼‰

**è§£å†³:**
- ä» VPC å†…éƒ¨éƒ¨ç½²
- æˆ–ä¸´æ—¶å¯ç”¨å…¬ç½‘:
```bash
# ä¿®æ”¹ manifests/cluster/eksctl_cluster_base.yaml
# publicAccess: false â†’ publicAccess: true
```

è¯¦ç»†è§£å†³æ–¹æ¡ˆè¯·å‚è€ƒä¸‹æ–¹çš„ [å¦‚ä½•ä» VPC å¤–éƒ¨ç½²é›†ç¾¤](#å¦‚ä½•ä»-vpc-å¤–éƒ¨ç½²é›†ç¾¤) ç« èŠ‚ã€‚

---

## å¦‚ä½•ä» VPC å¤–éƒ¨ç½²é›†ç¾¤

### èƒŒæ™¯è¯´æ˜

æœ¬é¡¹ç›®çš„ EKS é›†ç¾¤é‡‡ç”¨ **ç§æœ‰ API è®¿é—®æ¶æ„**ï¼ˆ`publicAccess: false`ï¼‰ï¼Œè¿™æ„å‘³ç€ï¼š

- âœ… **å®‰å…¨æ€§é«˜**ï¼šAPI Server ä»…åœ¨ VPC å†…éƒ¨å¯è®¿é—®ï¼Œä¸æš´éœ²åˆ°å…¬ç½‘
- âŒ **éƒ¨ç½²é™åˆ¶**ï¼šæ‰€æœ‰ kubectl å‘½ä»¤å¿…é¡»ä» VPC å†…éƒ¨æ‰§è¡Œ
- âŒ **CloudShell ä¸å¯ç”¨**ï¼šCloudShell è¿è¡Œåœ¨ AWS ç®¡ç†ç¯å¢ƒä¸­ï¼Œä¸åœ¨æ‚¨çš„ VPC å†…
- âŒ **æœ¬åœ°æœºå™¨ä¸å¯ç”¨**ï¼šé™¤éé€šè¿‡ VPN è¿æ¥åˆ° VPC

**éƒ¨ç½²æµç¨‹å¯¹æ¯”**ï¼š

```
âœ… VPC å†…éƒ¨ç½²ï¼ˆæ¨èï¼‰:
  EC2 (VPCå†…) â†’ EKS API (ç§æœ‰10.0.x.x) â†’ é›†ç¾¤éƒ¨ç½²æˆåŠŸ

âŒ VPC å¤–éƒ¨ç½²ï¼ˆä¼šå¤±è´¥ï¼‰:
  CloudShell/æœ¬åœ° â†’ [æ— æ³•è®¿é—®] â†’ EKS API (ç§æœ‰10.0.x.x) â†’ å¤±è´¥: dial tcp timeout

âš ï¸ ä¸´æ—¶å…¬ç½‘è®¿é—®:
  CloudShell/æœ¬åœ° â†’ Internet â†’ EKS API (å…¬ç½‘ä¸´æ—¶) â†’ é›†ç¾¤éƒ¨ç½²æˆåŠŸ â†’ ç¦ç”¨å…¬ç½‘
```

**å“ªäº›æ“ä½œå—å½±å“**ï¼š

| è„šæœ¬/æ“ä½œ | VPC å¤–å¯æ‰§è¡Œ | è¯´æ˜ |
|----------|------------|------|
| `0_setup_env.sh` | âœ… å¯ä»¥ | ä»…è®¾ç½®ç¯å¢ƒå˜é‡ |
| `1_enable_vpc_dns.sh` | âœ… å¯ä»¥ | AWS API æ“ä½œ |
| `2_validate_network_environment.sh` | âœ… å¯ä»¥ | AWS API éªŒè¯ |
| `3_create_vpc_endpoints.sh` | âœ… å¯ä»¥ | AWS API æ“ä½œ |
| `eksctl create cluster` | âœ… å¯ä»¥ | AWS æ‰˜ç®¡æ“ä½œ |
| `kubectl get nodes` | âŒ ä¸å¯ä»¥ | éœ€è¦è®¿é—®ç§æœ‰ API |
| `kubectl apply/helm install` | âŒ ä¸å¯ä»¥ | éœ€è¦è®¿é—®ç§æœ‰ API |
| æ‰€æœ‰ç»„ä»¶éƒ¨ç½² | âŒ ä¸å¯ä»¥ | éœ€è¦è®¿é—®ç§æœ‰ API |

---

### æ¨èæ–¹æ¡ˆï¼šä½¿ç”¨ä¸´æ—¶è·³æ¿æœºéƒ¨ç½²

#### æ–¹æ¡ˆä¼˜åŠ¿

- âœ… **å®‰å…¨**ï¼šAPI Server å§‹ç»ˆä¿æŒç§æœ‰ï¼Œä¸æš´éœ²åˆ°å…¬ç½‘
- âœ… **ç¬¦åˆæœ€ä½³å®è·µ**ï¼šç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
- âœ… **æˆæœ¬æä½**ï¼št3.micro è¿è¡Œ 30 åˆ†é’Ÿ < $0.01
- âœ… **æ— éœ€ SSH å¯†é’¥**ï¼šä½¿ç”¨ AWS Systems Manager Session Manager

> âš ï¸ **å‰ç½®è¦æ±‚**ï¼šå¦‚æœæ‚¨è®¡åˆ’å°† EC2 å®ä¾‹åˆ›å»ºåœ¨**ç§æœ‰å­ç½‘**ä¸­ï¼Œå¿…é¡»å…ˆè¿è¡Œ `./scripts/3_create_vpc_endpoints.sh` åˆ›å»º VPC ç«¯ç‚¹ï¼ˆåŒ…æ‹¬ SSM ç›¸å…³çš„ 3 ä¸ªç«¯ç‚¹ï¼š`ssm`ã€`ssmmessages`ã€`ec2messages`ï¼‰ã€‚å¦åˆ™ Session Manager å°†æ— æ³•è¿æ¥ã€‚

---

#### æ­¥éª¤ 0ï¼šåˆ›å»º VPC ç«¯ç‚¹ï¼ˆå¦‚æœå°šæœªåˆ›å»ºï¼‰

å¦‚æœæ‚¨å°šæœªåˆ›å»º VPC ç«¯ç‚¹ï¼Œè¯·å…ˆè¿è¡Œï¼š

```bash
# å¯ç”¨ VPC DNSï¼ˆå¿…éœ€ï¼‰
./scripts/1_enable_vpc_dns.sh

# åˆ›å»º VPC ç«¯ç‚¹ï¼ˆåŒ…å« SSM ç«¯ç‚¹ï¼‰
./scripts/3_create_vpc_endpoints.sh
```

è¿™å°†åˆ›å»º 13 ä¸ª VPC ç«¯ç‚¹ï¼ŒåŒ…æ‹¬ï¼š
- **EKS ç›¸å…³**ï¼šeksã€eks-authã€sts
- **å®¹å™¨é•œåƒ**ï¼šecr.apiã€ecr.dkr
- **æ—¥å¿—å’Œå­˜å‚¨**ï¼šlogsã€s3
- **EKS ç»„ä»¶**ï¼šec2ã€autoscalingã€elasticloadbalancingã€elasticfilesystem
- **Session Managerï¼ˆå…³é”®ï¼‰**ï¼šssmã€ssmmessagesã€ec2messages

ç­‰å¾… 2-3 åˆ†é’Ÿè®©ç«¯ç‚¹å˜ä¸ºå¯ç”¨çŠ¶æ€ã€‚

---

#### æ­¥éª¤ 1ï¼šåˆ›å»ºä¸´æ—¶ EC2 å®ä¾‹

> ğŸ’¡ **ç®€åŒ–æ–¹å¼**ï¼šä½¿ç”¨é¡¹ç›®æä¾›çš„è‡ªåŠ¨åŒ–è„šæœ¬ä¸€é”®åˆ›å»ºè·³æ¿æœºï¼š
> ```bash
> ./scripts/create_bastion.sh
> ```
> è¯¥è„šæœ¬ä¼šè‡ªåŠ¨å®Œæˆä»¥ä¸‹æ‰€æœ‰æ­¥éª¤ï¼ˆ1.1-1.5ï¼‰ï¼Œè·³åˆ°æ­¥éª¤ 2 è¿æ¥å®ä¾‹å³å¯ã€‚

**æ‰‹åŠ¨åˆ›å»ºæ­¥éª¤**ï¼ˆå¦‚æœä¸ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰ï¼š

**1.1 å‡†å¤‡é…ç½®**

é¦–å…ˆç¡®è®¤æ‚¨çš„ç¯å¢ƒå˜é‡ï¼ˆæ¥è‡ª `.env` æ–‡ä»¶ï¼‰ï¼š

```bash
# åŠ è½½ç¯å¢ƒå˜é‡
source scripts/0_setup_env.sh

# ç¡®è®¤å˜é‡
echo "VPC ID: $VPC_ID"
echo "Private Subnet: $PRIVATE_SUBNET_A"
```

**1.2 è·å–æœ€æ–°çš„ Amazon Linux 2023 AMI**

```bash
# è·å–æœ€æ–° AMI ID
AMI_ID=$(aws ec2 describe-images \
  --owners amazon \
  --filters "Name=name,Values=al2023-ami-2023.*-x86_64" \
            "Name=state,Values=available" \
  --query 'sort_by(Images, &CreationDate)[-1].ImageId' \
  --output text \
  --region ${AWS_DEFAULT_REGION})

echo "å°†ä½¿ç”¨ AMI: $AMI_ID"
```

**1.3 åˆ›å»ºæˆ–ç¡®è®¤ IAM è§’è‰²**

EC2 å®ä¾‹éœ€è¦ä»¥ä¸‹æƒé™ï¼š

```bash
# æ£€æŸ¥è§’è‰²æ˜¯å¦å­˜åœ¨
aws iam get-role --role-name EKS-Deploy-Role 2>/dev/null

# å¦‚æœä¸å­˜åœ¨ï¼Œåˆ›å»ºè§’è‰²
if [ $? -ne 0 ]; then
  echo "åˆ›å»º IAM è§’è‰²..."

  # åˆ›å»ºä¿¡ä»»ç­–ç•¥
  cat > /tmp/trust-policy.json <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
EOF

  # åˆ›å»ºè§’è‰²
  aws iam create-role \
    --role-name EKS-Deploy-Role \
    --assume-role-policy-document file:///tmp/trust-policy.json

  # é™„åŠ å¿…è¦æƒé™ï¼ˆæ ¹æ®æ‚¨çš„éœ€æ±‚è°ƒæ•´ï¼‰
  aws iam attach-role-policy \
    --role-name EKS-Deploy-Role \
    --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

  aws iam attach-role-policy \
    --role-name EKS-Deploy-Role \
    --policy-arn arn:aws:iam::aws:policy/AdministratorAccess  # ä»…ç”¨äºéƒ¨ç½²ï¼Œéƒ¨ç½²åå¯åˆ é™¤

  # åˆ›å»ºå®ä¾‹é…ç½®æ–‡ä»¶
  aws iam create-instance-profile --instance-profile-name EKS-Deploy-Profile
  aws iam add-role-to-instance-profile \
    --instance-profile-name EKS-Deploy-Profile \
    --role-name EKS-Deploy-Role

  # ç­‰å¾…è§’è‰²ç”Ÿæ•ˆ
  echo "ç­‰å¾… IAM è§’è‰²ç”Ÿæ•ˆ..."
  sleep 10
fi
```

**1.4 åˆ›å»ºå®‰å…¨ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰**

```bash
# åˆ›å»ºå®‰å…¨ç»„ï¼ˆä»…å…è®¸å‡ºç«™æµé‡ï¼ŒSession Manager ä¸éœ€è¦å…¥ç«™ï¼‰
SG_ID=$(aws ec2 create-security-group \
  --group-name eks-deploy-temp-sg \
  --description "Temporary SG for EKS deployment" \
  --vpc-id ${VPC_ID} \
  --output text \
  --region ${AWS_DEFAULT_REGION})

echo "åˆ›å»ºçš„å®‰å…¨ç»„ ID: $SG_ID"

# æ·»åŠ æ ‡ç­¾
aws ec2 create-tags \
  --resources $SG_ID \
  --tags Key=Name,Value=eks-deploy-temp-sg \
  --region ${AWS_DEFAULT_REGION}
```

**1.5 å¯åŠ¨ EC2 å®ä¾‹**

```bash
# åˆ›å»º EC2 å®ä¾‹
INSTANCE_ID=$(aws ec2 run-instances \
  --image-id ${AMI_ID} \
  --instance-type t3.micro \
  --subnet-id ${PUBLIC_SUBNET_A} \
  --security-group-ids ${SG_ID} \
  --iam-instance-profile Name=EKS-Deploy-Profile \
  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=EKS-Deploy-Temp},{Key=Purpose,Value=EKS-Deployment},{Key=AutoDelete,Value=true}]' \
  --region ${AWS_DEFAULT_REGION} \
  --query 'Instances[0].InstanceId' \
  --output text)

echo "å®ä¾‹ ID: $INSTANCE_ID"

# ç­‰å¾…å®ä¾‹è¿è¡Œ
echo "ç­‰å¾…å®ä¾‹å¯åŠ¨..."
aws ec2 wait instance-running --instance-ids $INSTANCE_ID --region ${AWS_DEFAULT_REGION}

# ç­‰å¾… SSM Agent å°±ç»ªï¼ˆå¤§çº¦ 1-2 åˆ†é’Ÿï¼‰
echo "ç­‰å¾… Systems Manager Agent å°±ç»ª..."
for i in {1..30}; do
  STATUS=$(aws ssm describe-instance-information \
    --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
    --query 'InstanceInformationList[0].PingStatus' \
    --output text \
    --region ${AWS_DEFAULT_REGION} 2>/dev/null)

  if [ "$STATUS" = "Online" ]; then
    echo "âœ… å®ä¾‹å·²å°±ç»ªï¼"
    break
  fi

  echo "ç­‰å¾…ä¸­... ($i/30)"
  sleep 10
done
```

**æˆæœ¬è¯´æ˜**ï¼št3.micro æŒ‰éœ€å®ä¾‹çº¦ $0.0104/å°æ—¶ï¼ˆus-east-1ï¼‰ï¼Œéƒ¨ç½²è€—æ—¶ 20-30 åˆ†é’Ÿï¼Œæ€»æˆæœ¬ä¸åˆ° $0.01ã€‚

---

#### æ­¥éª¤ 2ï¼šè¿æ¥åˆ°å®ä¾‹

**ä½¿ç”¨ AWS Systems Manager Session Managerï¼ˆæ¨èï¼‰**ï¼š

```bash
# å¦‚æœä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬åˆ›å»ºï¼Œå®ä¾‹ ID å·²ä¿å­˜
INSTANCE_ID=$(cat /tmp/eks-bastion-instance-id.txt)

# æ–¹å¼ 1ï¼šé€šè¿‡ AWS CLI è¿æ¥
aws ssm start-session \
  --target $INSTANCE_ID \
  --region ${AWS_DEFAULT_REGION}

# æ–¹å¼ 2ï¼šé€šè¿‡ AWS æ§åˆ¶å°è¿æ¥
# è®¿é—® EC2 æ§åˆ¶å° â†’ é€‰æ‹©å®ä¾‹ â†’ ç‚¹å‡»"è¿æ¥" â†’ é€‰æ‹©"Session Manager"æ ‡ç­¾é¡µ â†’ ç‚¹å‡»"è¿æ¥"
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€ SSH å¯†é’¥
- âœ… æ— éœ€å¼€æ”¾ 22 ç«¯å£
- âœ… æ‰€æœ‰ä¼šè¯è®°å½•åœ¨ CloudTrail
- âœ… å¯é€šè¿‡ IAM ç²¾ç»†æ§åˆ¶è®¿é—®æƒé™

è¿æ¥æˆåŠŸåï¼Œæ‚¨å°†çœ‹åˆ°ç±»ä¼¼çš„æç¤ºç¬¦ï¼š

```
sh-5.2$
```

---

#### æ­¥éª¤ 3ï¼šåœ¨å®ä¾‹ä¸Šå®‰è£…å¿…è¦å·¥å…·

> ğŸ’¡ **ç®€åŒ–æ–¹å¼**ï¼šä½¿ç”¨é¡¹ç›®æä¾›çš„è‡ªåŠ¨åŒ–è„šæœ¬ä¸€é”®å®‰è£…æ‰€æœ‰å·¥å…·ï¼š
> ```bash
> # ä» GitHub ä¸‹è½½å®‰è£…è„šæœ¬
> curl -O https://raw.githubusercontent.com/your-username/eks-cluster-deployment/main/scripts/install_tools.sh
> bash install_tools.sh
> ```
>
> æˆ–è€…å¦‚æœå·²ç»å…‹éš†äº†é¡¹ç›®ï¼š
> ```bash
> cd eks-cluster-deployment
> ./scripts/install_tools.sh
> ```

**æ‰‹åŠ¨å®‰è£…æ­¥éª¤**ï¼ˆå¦‚æœä¸ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰ï¼š

è¿æ¥åˆ°å®ä¾‹åï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€æœ‰å¿…è¦å·¥å…·ï¼š

```bash
#!/bin/bash
# ä¸€é”®å®‰è£…æ‰€æœ‰éƒ¨ç½²å·¥å…·

echo "=== å®‰è£… EKS éƒ¨ç½²å·¥å…· ==="

# æ›´æ–°ç³»ç»Ÿ
sudo yum update -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo yum install -y git unzip tar gzip jq

# 1. å®‰è£… kubectl
echo "å®‰è£… kubectl..."
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
rm kubectl

# 2. å®‰è£… eksctl
echo "å®‰è£… eksctl..."
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin
chmod +x /usr/local/bin/eksctl

# 3. å®‰è£… helm
echo "å®‰è£… helm..."
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# 4. éªŒè¯å®‰è£…
echo ""
echo "=== éªŒè¯å·¥å…·ç‰ˆæœ¬ ==="
kubectl version --client
eksctl version
helm version
aws --version

echo ""
echo "âœ… æ‰€æœ‰å·¥å…·å®‰è£…å®Œæˆï¼"
```

**é¢„æœŸè¾“å‡º**ï¼š

```
Client Version: v1.31.x
eksctl version: 0.x.x
version.BuildInfo{Version:"v3.x.x"...}
aws-cli/2.x.x Python/3.x.x Linux/6.x.x
```

---

#### æ­¥éª¤ 4ï¼šå…‹éš†é¡¹ç›®å¹¶è¿è¡Œå®‰è£…è„šæœ¬

**4.1 å…‹éš†é¡¹ç›®ä»£ç **

å¦‚æœé¡¹ç›®åœ¨ Git ä»“åº“ä¸­ï¼š

```bash
# å…‹éš†é¡¹ç›®
cd ~
git clone <your-repository-url> eks-cluster-deployment
cd eks-cluster-deployment

# æˆ–è€…ï¼Œå¦‚æœéœ€è¦è®¤è¯
git clone https://github.com/your-username/eks-cluster-deployment.git
```

å¦‚æœé¡¹ç›®ä¸åœ¨ Git ä»“åº“ï¼Œå¯ä»¥ä»æœ¬åœ°ä¸Šä¼ ï¼š

```bash
# åœ¨æœ¬åœ°æœºå™¨ä¸Šæ‰“åŒ…
tar czf eks-project.tar.gz eks-cluster-deployment/

# ä¸Šä¼ åˆ° S3
aws s3 cp eks-project.tar.gz s3://your-bucket/

# åœ¨ EC2 å®ä¾‹ä¸Šä¸‹è½½
aws s3 cp s3://your-bucket/eks-project.tar.gz .
tar xzf eks-project.tar.gz
cd eks-cluster-deployment
```

**4.2 é…ç½®ç¯å¢ƒå˜é‡**

```bash
# å¤åˆ¶å¹¶ç¼–è¾‘é…ç½®æ–‡ä»¶
cp .env.example .env
nano .env  # æˆ–ä½¿ç”¨ vi

# ç¡®ä¿å¡«å†™æ­£ç¡®çš„å€¼ï¼š
# - CLUSTER_NAME
# - VPC_ID
# - æ‰€æœ‰å­ç½‘ ID
# - AWS_REGION
```

**4.3 è¿è¡Œå®‰è£…è„šæœ¬**

```bash
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x scripts/*.sh

# è¿è¡Œå®Œæ•´å®‰è£…
./scripts/4_install_eks_cluster.sh

# æˆ–è€…åˆ†æ­¥æ‰§è¡Œ
./scripts/1_enable_vpc_dns.sh
./scripts/2_validate_network_environment.sh
./scripts/3_create_vpc_endpoints.sh
./scripts/4_install_eks_cluster.sh
```

**éƒ¨ç½²æ—¶é—´**ï¼šçº¦ 20-25 åˆ†é’Ÿ

**ç›‘æ§éƒ¨ç½²è¿›åº¦**ï¼š

```bash
# æŸ¥çœ‹é›†ç¾¤åˆ›å»ºçŠ¶æ€
eksctl get cluster --name ${CLUSTER_NAME} --region ${AWS_DEFAULT_REGION}

# æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€
kubectl get nodes

# æŸ¥çœ‹æ‰€æœ‰ Pod
kubectl get pods -A
```

---

#### æ­¥éª¤ 5ï¼šæ¸…ç†ä¸´æ—¶èµ„æº

éƒ¨ç½²å®Œæˆå¹¶éªŒè¯é›†ç¾¤æ­£å¸¸åï¼Œæ¸…ç†ä¸´æ—¶ EC2 å®ä¾‹ï¼š

> ğŸ’¡ **ç®€åŒ–æ–¹å¼**ï¼šä½¿ç”¨é¡¹ç›®æä¾›çš„è‡ªåŠ¨åŒ–è„šæœ¬åˆ é™¤è·³æ¿æœºï¼š
> ```bash
> ./scripts/delete_bastion.sh
> ```
> è¯¥è„šæœ¬ä¼šè‡ªåŠ¨æŸ¥æ‰¾å¹¶åˆ é™¤è·³æ¿æœºå®ä¾‹ã€‚

**æ‰‹åŠ¨æ¸…ç†æ­¥éª¤**ï¼ˆå¦‚æœä¸ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼‰ï¼š

**5.1 é€€å‡º Session Manager**

```bash
# åœ¨ EC2 å®ä¾‹çš„ shell ä¸­æ‰§è¡Œ
exit
```

**5.2 ç»ˆæ­¢ EC2 å®ä¾‹**

```bash
# åœ¨æœ¬åœ°æˆ– CloudShell ä¸­æ‰§è¡Œ

# è·å–å®ä¾‹ IDï¼ˆå¦‚æœä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬åˆ›å»ºï¼‰
INSTANCE_ID=$(cat /tmp/eks-bastion-instance-id.txt)

# æˆ–æ‰‹åŠ¨æŸ¥æ‰¾
# INSTANCE_ID=$(aws ec2 describe-instances \
#   --filters "Name=tag:Name,Values=EKS-Deploy-Bastion-${CLUSTER_NAME}" \
#   --query 'Reservations[0].Instances[0].InstanceId' \
#   --output text)

# ç»ˆæ­¢å®ä¾‹
aws ec2 terminate-instances \
  --instance-ids $INSTANCE_ID \
  --region ${AWS_DEFAULT_REGION}

# éªŒè¯å®ä¾‹å·²ç»ˆæ­¢
aws ec2 describe-instances \
  --instance-ids $INSTANCE_ID \
  --query 'Reservations[0].Instances[0].State.Name' \
  --output text \
  --region ${AWS_DEFAULT_REGION}
```

**5.3 æ¸…ç†å®‰å…¨ç»„å’Œ IAM èµ„æºï¼ˆå¯é€‰ï¼‰**

```bash
# ç­‰å¾…å®ä¾‹å®Œå…¨ç»ˆæ­¢åï¼Œåˆ é™¤å®‰å…¨ç»„
aws ec2 delete-security-group \
  --group-id $SG_ID \
  --region ${AWS_DEFAULT_REGION}

# å¦‚æœä¸å†éœ€è¦ï¼Œåˆ é™¤ IAM è§’è‰²
aws iam remove-role-from-instance-profile \
  --instance-profile-name EKS-Deploy-Profile \
  --role-name EKS-Deploy-Role

aws iam delete-instance-profile \
  --instance-profile-name EKS-Deploy-Profile

aws iam detach-role-policy \
  --role-name EKS-Deploy-Role \
  --policy-arn arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore

aws iam detach-role-policy \
  --role-name EKS-Deploy-Role \
  --policy-arn arn:aws:iam::aws:policy/AdministratorAccess

aws iam delete-role --role-name EKS-Deploy-Role
```

---

### å¤‡é€‰æ–¹æ¡ˆï¼šä¸´æ—¶å¯ç”¨å…¬ç½‘è®¿é—®

å¦‚æœæ‚¨æ›´å€¾å‘äºä» CloudShell æˆ–æœ¬åœ°æœºå™¨ç›´æ¥éƒ¨ç½²ï¼ˆé€‚ç”¨äºå¼€å‘/æµ‹è¯•ç¯å¢ƒï¼‰ï¼Œå¯ä»¥ä¸´æ—¶å¯ç”¨å…¬ç½‘è®¿é—®ã€‚

#### æ–¹æ¡ˆä¼˜åŠ¿

- âœ… **ç®€å•å¿«æ·**ï¼šæ— éœ€åˆ›å»ºé¢å¤–èµ„æº
- âœ… **é›¶æˆæœ¬**ï¼šä½¿ç”¨ CloudShell å®Œå…¨å…è´¹
- âš ï¸ **å®‰å…¨æ€§è¾ƒä½**ï¼šä¸´æ—¶æš´éœ² API Server åˆ°å…¬ç½‘

#### å®æ–½æ­¥éª¤

**æ­¥éª¤ 1ï¼šä¿®æ”¹é›†ç¾¤é…ç½®**

ç¼–è¾‘ `manifests/cluster/eksctl_cluster_base.yaml`:

```yaml
clusterEndpoints:
  privateAccess: true
  publicAccess: true        # ä¿®æ”¹ä¸º true
  publicAccessCIDRs:        # å¯é€‰ï¼šé™åˆ¶è®¿é—® IP
    - "YOUR_IP/32"          # æ›¿æ¢ä¸ºæ‚¨çš„å…¬ç½‘ IP
```

**è·å–æ‚¨çš„å…¬ç½‘ IP**ï¼š

```bash
curl ifconfig.me
# æˆ–
curl checkip.amazonaws.com
```

**æ­¥éª¤ 2ï¼šåœ¨ CloudShell ä¸­è¿è¡Œéƒ¨ç½²**

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo> eks-cluster-deployment
cd eks-cluster-deployment

# é…ç½®ç¯å¢ƒ
cp .env.example .env
nano .env

# è¿è¡Œå®‰è£…
./scripts/4_install_eks_cluster.sh
```

**æ­¥éª¤ 3ï¼šéƒ¨ç½²å®Œæˆåç¦ç”¨å…¬ç½‘è®¿é—®**

é€‰é¡¹ Aï¼šä½¿ç”¨ AWS CLIï¼ˆä»ä»»ä½•åœ°æ–¹ï¼‰

```bash
aws eks update-cluster-config \
  --name ${CLUSTER_NAME} \
  --resources-vpc-config endpointPublicAccess=false,endpointPrivateAccess=true \
  --region ${AWS_DEFAULT_REGION}

# ç­‰å¾…æ›´æ–°å®Œæˆï¼ˆçº¦ 5 åˆ†é’Ÿï¼‰
aws eks wait cluster-active \
  --name ${CLUSTER_NAME} \
  --region ${AWS_DEFAULT_REGION}
```

é€‰é¡¹ Bï¼šä½¿ç”¨é¡¹ç›®æä¾›çš„è„šæœ¬ï¼ˆéœ€è¦ä» VPC å†…æˆ–å…¬ç½‘è®¿é—®ä»ç„¶å¯ç”¨æ—¶ï¼‰

```bash
./scripts/disable_public_access.sh
```

---

### æ•…éšœæ’æŸ¥

#### é—®é¢˜ 1ï¼šSession Manager æ— æ³•è¿æ¥

**ç—‡çŠ¶**ï¼š`aws ssm start-session` è¿”å›é”™è¯¯æˆ–è¶…æ—¶

**æ’æŸ¥æ­¥éª¤**ï¼š

```bash
# 1. ç¡®è®¤å®ä¾‹çŠ¶æ€
aws ec2 describe-instance-status --instance-ids $INSTANCE_ID

# 2. ç¡®è®¤ SSM Agent çŠ¶æ€
aws ssm describe-instance-information \
  --filters "Key=InstanceIds,Values=$INSTANCE_ID"

# 3. æ£€æŸ¥ IAM è§’è‰²æ˜¯å¦æ­£ç¡®é™„åŠ 
aws ec2 describe-instances \
  --instance-ids $INSTANCE_ID \
  --query 'Reservations[0].Instances[0].IamInstanceProfile'

# 4. æ£€æŸ¥ VPC ç«¯ç‚¹ï¼ˆå¦‚æœä½¿ç”¨ç§æœ‰å­ç½‘ï¼‰
aws ec2 describe-vpc-endpoints --filters "Name=vpc-id,Values=$VPC_ID"
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ç­‰å¾… 2-3 åˆ†é’Ÿè®© SSM Agent å®Œå…¨åˆå§‹åŒ–
- ç¡®è®¤ IAM è§’è‰²åŒ…å« `AmazonSSMManagedInstanceCore` ç­–ç•¥
- **å¦‚æœä½¿ç”¨ç§æœ‰å­ç½‘ï¼Œå¿…é¡»ç¡®ä¿ VPC æœ‰ä»¥ä¸‹ 3 ä¸ªç«¯ç‚¹**ï¼š
  - `com.amazonaws.<region>.ssm` - Systems Manager ç«¯ç‚¹
  - `com.amazonaws.<region>.ssmmessages` - Session Manager æ¶ˆæ¯ç«¯ç‚¹
  - `com.amazonaws.<region>.ec2messages` - EC2 æ¶ˆæ¯ç«¯ç‚¹ï¼ˆç”¨äº SSM Agentï¼‰

**é‡è¦æç¤º**ï¼šæœ¬é¡¹ç›®çš„ `scripts/3_create_vpc_endpoints.sh` è„šæœ¬å·²ç»åŒ…å«äº†è¿™ 3 ä¸ª SSM ç«¯ç‚¹ã€‚å¦‚æœæ‚¨çš„ VPC ç«¯ç‚¹æ˜¯æ‰‹åŠ¨åˆ›å»ºçš„æˆ–ä½¿ç”¨æ—§ç‰ˆæœ¬è„šæœ¬ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤è¡¥å……åˆ›å»ºï¼š

```bash
# é‡æ–°è¿è¡Œ VPC ç«¯ç‚¹åˆ›å»ºè„šæœ¬ï¼ˆä¼šè·³è¿‡å·²å­˜åœ¨çš„ç«¯ç‚¹ï¼‰
./scripts/3_create_vpc_endpoints.sh

# æˆ–æ‰‹åŠ¨åˆ›å»ºç¼ºå¤±çš„ SSM ç«¯ç‚¹
source scripts/0_setup_env.sh

# åˆ›å»º ssmmessages ç«¯ç‚¹
aws ec2 create-vpc-endpoint \
  --vpc-id ${VPC_ID} \
  --service-name com.amazonaws.${AWS_REGION}.ssmmessages \
  --vpc-endpoint-type Interface \
  --subnet-ids ${PRIVATE_SUBNET_A} ${PRIVATE_SUBNET_B} ${PRIVATE_SUBNET_C} \
  --security-group-ids <your-vpc-endpoints-sg-id> \
  --private-dns-enabled

# åˆ›å»º ec2messages ç«¯ç‚¹
aws ec2 create-vpc-endpoint \
  --vpc-id ${VPC_ID} \
  --service-name com.amazonaws.${AWS_REGION}.ec2messages \
  --vpc-endpoint-type Interface \
  --subnet-ids ${PRIVATE_SUBNET_A} ${PRIVATE_SUBNET_B} ${PRIVATE_SUBNET_C} \
  --security-group-ids <your-vpc-endpoints-sg-id> \
  --private-dns-enabled
```

#### é—®é¢˜ 2ï¼škubectl æç¤ºæƒé™ä¸è¶³

**ç—‡çŠ¶**ï¼š`error: You must be logged in to the server (Unauthorized)`

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ›´æ–° kubeconfig
aws eks update-kubeconfig \
  --name ${CLUSTER_NAME} \
  --region ${AWS_DEFAULT_REGION}

# éªŒè¯é…ç½®
kubectl config current-context
kubectl get nodes
```

#### é—®é¢˜ 3ï¼šå·¥å…·å®‰è£…å¤±è´¥

**ç—‡çŠ¶**ï¼škubectl/eksctl/helm å®‰è£…é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping -c 3 google.com

# å¦‚æœåœ¨ç§æœ‰å­ç½‘ï¼Œæ£€æŸ¥ NAT Gateway
aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$VPC_ID"

# æ£€æŸ¥è·¯ç”±è¡¨
aws ec2 describe-route-tables --filters "Name=vpc-id,Values=$VPC_ID"
```

---

### æœ€ä½³å®è·µå»ºè®®

1. **ç”Ÿäº§ç¯å¢ƒ**ï¼š
   - âœ… ä½¿ç”¨ä¸´æ—¶è·³æ¿æœºæ–¹æ¡ˆ
   - âœ… ä¿æŒ API Server ç§æœ‰è®¿é—®
   - âœ… ä½¿ç”¨ Session Manager è€Œé SSH
   - âœ… éƒ¨ç½²å®Œæˆç«‹å³åˆ é™¤è·³æ¿æœº

2. **å¼€å‘/æµ‹è¯•ç¯å¢ƒ**ï¼š
   - âœ… å¯ä»¥ä¸´æ—¶å¯ç”¨å…¬ç½‘è®¿é—®
   - âœ… ä½¿ç”¨ IP ç™½åå•é™åˆ¶è®¿é—®
   - âœ… éƒ¨ç½²å®Œæˆåç¦ç”¨å…¬ç½‘è®¿é—®

3. **é•¿æœŸç»´æŠ¤**ï¼š
   - è€ƒè™‘è®¾ç½®æ°¸ä¹…è·³æ¿æœºï¼ˆä½¿ç”¨è‡ªåŠ¨å…³æœºç­–ç•¥é™ä½æˆæœ¬ï¼‰
   - æˆ–é…ç½® AWS Client VPN
   - æˆ–ä½¿ç”¨ AWS Direct Connect / Site-to-Site VPN

4. **å®‰å…¨å»ºè®®**ï¼š
   - âŒ ä¸è¦é•¿æœŸå¯ç”¨ API Server å…¬ç½‘è®¿é—®
   - âœ… ä½¿ç”¨ IAM è§’è‰²è€Œéé•¿æœŸå¯†é’¥
   - âœ… å®šæœŸå®¡è®¡ CloudTrail æ—¥å¿—
   - âœ… ä½¿ç”¨ Security Groups å’Œ Network ACLs åŠ å›ºç½‘ç»œ

---

### é—®é¢˜ 3: Pod æ— æ³•è°ƒåº¦åˆ° app èŠ‚ç‚¹

**åŸå› :** ç¼ºå°‘ Toleration

**è§£å†³:**
```yaml
spec:
  tolerations:
  - key: "workload"
    operator: "Equal"
    value: "user-apps"
    effect: "NoSchedule"
  nodeSelector:
    workload: user-apps
```

### é—®é¢˜ 4: SSH Key ä¸å­˜åœ¨

```bash
# åˆ›å»º
aws ec2 create-key-pair --key-name spider --region ap-southeast-1 \
  --query 'KeyMaterial' --output text > spider.pem
chmod 400 spider.pem
```

### é—®é¢˜ 5: æ•°æ®ç›˜æœªæŒ‚è½½

```bash
# SSH åˆ°èŠ‚ç‚¹
aws ssm start-session --target <instance-id>

# æŸ¥çœ‹ç£ç›˜
lsblk

# æ£€æŸ¥æ—¥å¿—
sudo cat /var/log/spider-node-init.log

# æ‰‹åŠ¨æŒ‚è½½
sudo mkfs -t xfs /dev/xvdb
sudo mount /dev/xvdb /data
```

---

## ğŸ—‘ï¸ æ¸…ç†èµ„æº

### å®Œæ•´æ¸…ç†

```bash
# 1. åˆ é™¤æµ‹è¯•åº”ç”¨
kubectl delete deployment test 2>/dev/null || true
kubectl delete namespace game-2048 2>/dev/null || true

# 2. åˆ é™¤ Load Balancer
kubectl delete ingress --all -A

# 3. åˆ é™¤ PVC
kubectl delete pvc --all -A

# 4. ç­‰å¾…
sleep 60

# 5. åˆ é™¤é›†ç¾¤
eksctl delete cluster --name=${CLUSTER_NAME} --region=${AWS_REGION} --wait
```

### æ¸…ç† Launch Template

```bash
cd terraform/launch-template
terraform destroy
```

### æ¸…ç† VPC

```bash
cd terraform/vpc
terraform destroy
```

---

## ğŸ“Š é¡¹ç›®ç»“æ„

```
eks-cluster-deployment/
â”œâ”€â”€ README.md                    # æœ¬æ–‡æ¡£ï¼ˆå”¯ä¸€æ–‡æ¡£ï¼‰
â”œâ”€â”€ .env.example                 # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 0_setup_env.sh          # ç¯å¢ƒå˜é‡åŠ è½½
â”‚   â”œâ”€â”€ 4_install_eks_cluster.sh            # æ ‡å‡†éƒ¨ç½²
â”‚   â””â”€â”€ 6_install_eks_with_custom_nodegroup.sh  # Launch Template éƒ¨ç½²
â”‚
â”œâ”€â”€ manifests/
â”‚   â”œâ”€â”€ cluster/
â”‚   â”‚   â”œâ”€â”€ eksctl_cluster_template.yaml    # åŸå§‹ï¼ˆä¸¤ä¸ªèŠ‚ç‚¹ç»„ï¼‰
â”‚   â”‚   â”œâ”€â”€ eksctl_cluster_base.yaml        # åŸºç¡€ï¼ˆä»… eks-utilsï¼‰
â”‚   â”‚   â””â”€â”€ eksctl_nodegroup_app.yaml       # app èŠ‚ç‚¹ç»„
â”‚   â”œâ”€â”€ addons/
â”‚   â”‚   â”œâ”€â”€ cluster-autoscaler-rbac.yaml
â”‚   â”‚   â”œâ”€â”€ cluster-autoscaler.yaml
â”‚   â”‚   â”œâ”€â”€ efs-csi-driver.yaml
â”‚   â”‚   â””â”€â”€ s3-csi-driver.yaml
â”‚   â””â”€â”€ examples/
â”‚       â”œâ”€â”€ autoscaler.yaml
â”‚       â”œâ”€â”€ ebs-app.yaml
â”‚       â”œâ”€â”€ efs-app.yaml
â”‚       â””â”€â”€ s3-app.yaml
â”‚
â””â”€â”€ terraform/
    â”œâ”€â”€ vpc/                     # VPC åˆ›å»º
    â”œâ”€â”€ vpc-endpoints/           # VPC Endpoints
    â””â”€â”€ launch-template/         # Launch Template
        â”œâ”€â”€ main.tf
        â”œâ”€â”€ variables.tf
        â”œâ”€â”€ outputs.tf
        â””â”€â”€ terraform.tfvars.spider-example  # Spider ç¤ºä¾‹
```

---

## ğŸ“š å¸¸ç”¨å‘½ä»¤

### é›†ç¾¤ç®¡ç†
```bash
# æŸ¥çœ‹é›†ç¾¤
eksctl get cluster --region=${AWS_REGION}

# æ›´æ–° kubeconfig
aws eks update-kubeconfig --name ${CLUSTER_NAME} --region ${AWS_REGION}

# èŠ‚ç‚¹ç»„
eksctl get nodegroup --cluster=${CLUSTER_NAME} --region=${AWS_REGION}
```

### èŠ‚ç‚¹ç®¡ç†
```bash
# åˆ—å‡ºèŠ‚ç‚¹
kubectl get nodes -o wide

# è¯¦æƒ…
kubectl describe node <node-name>

# èµ„æºä½¿ç”¨
kubectl top nodes
```

### ç›‘æ§
```bash
# Pod èµ„æº
kubectl top pods -A --sort-by=memory

# äº‹ä»¶
kubectl get events -A --sort-by='.lastTimestamp' | tail -20

# æ—¥å¿—
kubectl logs -n kube-system -l app=cluster-autoscaler -f
kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -f
```

### Launch Template
```bash
# æŸ¥çœ‹
aws ec2 describe-launch-templates --region ${AWS_REGION}

# æ›´æ–°
cd terraform/launch-template
terraform apply

# æ›´æ–°èŠ‚ç‚¹ç»„
eksctl upgrade nodegroup --cluster=${CLUSTER_NAME} --name=app --region=${AWS_REGION}
```

---

## ğŸ†˜ è·å–å¸®åŠ©

### æ–‡æ¡£
- [AWS EKS](https://docs.aws.amazon.com/eks/)
- [eksctl](https://eksctl.io/)
- [Kubernetes](https://kubernetes.io/docs/)
- [Cluster Autoscaler](https://github.com/kubernetes/autoscaler)

### æ’æŸ¥æµç¨‹
1. `kubectl describe pod <pod-name>`
2. `kubectl logs <pod-name>`
3. `kubectl describe node <node-name>`
4. æŸ¥çœ‹ CloudFormation
5. æŸ¥çœ‹ CloudWatch Logs

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v2.0 (2025-12-09)
- âœ… æ·»åŠ  Launch Template æ”¯æŒ
- âœ… æ”¯æŒè‡ªå®šä¹‰ SSH Keyã€æ•°æ®ç›˜ã€User Data
- âœ… æ·»åŠ  Spider çˆ¬è™«é¡¹ç›®ç¤ºä¾‹
- âœ… ç»Ÿä¸€æ–‡æ¡£ï¼Œåˆ é™¤å†—ä½™æ–‡ä»¶
- âœ… æ›´æ–°æ‰€æœ‰é…ç½®ä¸ºæœ€æ–°ç‰ˆæœ¬

### v1.0 (2025-12-05)
- âœ… åˆå§‹ç‰ˆæœ¬
- âœ… æ··åˆæ¶æ„ï¼ˆIntel + Gravitonï¼‰
- âœ… Cluster Autoscaler
- âœ… AWS Load Balancer Controller
- âœ… EBS/EFS/S3 CSI Driver

---

**ç»´æŠ¤è€…:** Platform Team
**æœ€åæ›´æ–°:** 2025-12-09
**æ–‡æ¡£ç‰ˆæœ¬:** v2.0

---

## ğŸš¨ éƒ¨ç½²æ‰§è¡Œè®°å½•

### æ–°åŠ å¡é›†ç¾¤éƒ¨ç½² (2025-12-09)

**é›†ç¾¤ä¿¡æ¯**:
- åç§°: eks-singapore
- åŒºåŸŸ: ap-southeast-1
- çŠ¶æ€: âœ… ACTIVE
- èŠ‚ç‚¹: 6ä¸ª (3x m7i.large + 3x c8g.large)
- éƒ¨ç½²æ—¶é—´: çº¦13åˆ†é’Ÿ

**å ¡å’æœº**:
- å®ä¾‹ID: i-0b3bc4cfb8b84e34c
- å­ç½‘: subnet-0b3ff3647c930a34e
- ç”¨é€”: VPCå†…éƒ¨éƒ¨ç½²é›†ç¾¤

### kubectl é…ç½®é‡è¦æç¤º

**é—®é¢˜**: kubectl å°è¯•è¿æ¥ localhost:8080

**åŸå› **: `KUBECONFIG` ç¯å¢ƒå˜é‡æœªè®¾ç½®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åœ¨ä»»ä½•ä½¿ç”¨kubectlçš„è„šæœ¬ä¸­,æ·»åŠ :
export KUBECONFIG="${HOME}/.kube/config"

# æˆ–åœ¨å‘½ä»¤ä¸­æŒ‡å®š:
kubectl --kubeconfig=/root/.kube/config get nodes
```

**æœ€ä½³å®è·µ**: 
- å§‹ç»ˆåœ¨è„šæœ¬å¼€å¤´æ˜¾å¼è®¾ç½® `KUBECONFIG`
- å¯¹ç§æœ‰é›†ç¾¤,ä½¿ç”¨ `timeout` é¿å…é•¿æ—¶é—´ç­‰å¾…
- æä¾› AWS CLI å¤‡ç”¨éªŒè¯æ–¹æ¡ˆ

è¯¦ç»†è¯´æ˜è§éƒ¨ç½²è„šæœ¬: [scripts/working_deploy_eks.sh](scripts/working_deploy_eks.sh)

